"""


GWAS pipeline 


Requirements for input:


1. To remain sensible, it would be required to proivde a "sample sheet" in tsv format.
   There should at least be two columns: 
   	sample, file
   The read orientation would be inferred.  For now, we assume that the output from
   different lanes are merged - will extend it

2. If our input is already bam - for now i think it is better to extract fastq reads
   and realign so there are no software artifacts.


"""

import pandas as pd
import os

from scripts import dumbrename

from glob import glob

import pathlib

for i in [1,2]:
	pass
# Move params to a config file - keep like this for now



hapmap_resource_vcf 			= "assets/vcf/hapmap_3.3.hg38.vcf.gz"
dbsnp_resource_vcf   			= "assets/vcf/Homo_sapiens_assembly38.dbsnp138.vcf"
one_thousand_genomes_resource_vcf 	= "assets/vcf/1000G_omni2.5.hg38.vcf.gz"
omni_resource_vcf 			= "assets/vcf/1000G_omni2.5.hg38.vcf.gz"
bwaindex				= "assets/bwaindex/Homo_sapiens_assembly38.fasta.64"

# Check the dbsnp ref
dbsnp_known_sites_vcf = "assets/vcf/Homo_sapiens_assembly38.dbsnp138.vcf" 
mills_and_1000_gold_standard = "assets/vcf/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz"
hg38_known_indels = "assets/vcf/Homo_sapiens_assembly38.known_indels.vcf.gz"






config["inputdir"] = None if "inputdir" not in config else config["inputdir"]
config["sampletable"] = None if "sampletable" not in config else config["sampletable"]

print(config["inputdir"])

def makeoutputs():
        global config

        if config["inputdir"] is not None:
                fastafiles = glob("%s/*"%config["inputdir"])

        if config["sampletable"] is not None:
                sample_dataframe = pd.read_csv(config["sampletable"]).set_index("prefix")
                sample_dataframe["sample"] = sample_dataframe.index.values
                return sample_dataframe

	
        sample_dataframe = dumbrename.getFastqPrefixes(fastafiles,dohex=True, test_num_delim=4,getmaxlen=True)
	
	
	sample_dataframe["sample"] = sample_dataframe.index.values	
	
        return sample_dataframe





# Should move this to a proper config file

sample_dataframe = makeoutputs()

sample_paired_end = sample_dataframe[sample_dataframe.counts == 2]
sample_single_end = sample_dataframe[sample_dataframe.counts == 1]

all_samples = sample_dataframe["sample"].drop_duplicates().values

print(sample_paired_end)
print(sample_single_end)

print(sample_dataframe)

# Placeholder for now 
rule all:
	input:
		trim_single=expand("output/trimmed/single_end/{sample_single}_trimmed.fq.gz",sample_single=sample_single_end["sample"].values),
		trim_paired=expand("output/trimmed/paired_end/{sample_paired}_val_{num}.fq.gz",sample_paired=sample_paired_end["sample"].drop_duplicates().values,num=[1,2]),
		bam_single=expand("output/mapping/single_end/{sample_single}.bam",sample_single=sample_single_end["sample"].drop_duplicates().values),
		bam_paired=expand("output/mapping/paired_end/{sample_paired}.bam",sample_paired=sample_paired_end["sample"].drop_duplicates().values),
		bam_gather=expand("output/mapping/{sample}.bam",sample=sample_dataframe["sample"].drop_duplicates().values),

		#bam_downsample=expand("output/mapping/{sample}.downsample.bam",sample=all_samples)

		bqsr = expand("output/bqsr/{sample}.bqsr", sample=all_samples),
		


print(expand("output/trimmed/{sample}_U.fq.gz",sample=sample_single_end["sample"].values))
print(expand("output/trimmed/{sample}_R{num}.fq.gz",sample=sample_paired_end["sample"].drop_duplicates().values,num=[1,2]))

# Pretty ugly - but it willl work for now
rule trimming_single:
	input:
		R=lambda wildcards: sample_single_end[sample_single_end["sample"] == wildcards.sample_single].fastq

	output:
		R="output/trimmed/single_end/{sample_single}_trimmed.fq.gz"
	
	threads: 4
	
	params:
		folder="output/trimmed/single_end",
		basename="{sample_single}"

	log: "output/log/{sample_single}_trimming.log"
	shell:
		"trim_galore --cores {threads} {input.R} -o {params.folder} --basename {params.basename} > {log} 2> {log}"
rule trimming:
	input: 
		R=lambda wildcards: sample_paired_end[sample_paired_end["sample"] == wildcards.sample_paired].fastq.values,

	output: 
		FR="output/trimmed/paired_end/{sample_paired}_val_1.fq.gz",
		RR="output/trimmed/paired_end/{sample_paired}_val_2.fq.gz",
	log: "output/log/{sample_paired}_trimming.log"

	params:
		folder="output/trimmed/paired_end",
		basename="{sample_paired}"
	threads: 4
	shell:
		"trim_galore --cores {threads} --paired {input.R} -o {params.folder} --basename {params.basename} > {log} 2> {log}"
# Not the correct way - need to configure for .alt files too

rule bwa_mapping_single:
	input:
		R="output/trimmed/single_end/{sample_single}_trimmed.fq.gz"
	output:
		bam="output/mapping/single_end/{sample_single}.bam",
		bam_unmapped="output/mapping/single_end/{sample_single}_unmapped.bam"
	params: index=bwaindex,
		sample=lambda wildcards: wildcards.sample_single
	threads: 4
	shell:
		"bwa mem -K 100000000 -Y {params.index} -t {threads} -R '@RG\\tID:{params.sample}\\tSM:{params.sample}'  {input} |"
		"tee >(samtools view -f 4 -o {output.bam_unmapped}) >(samtools view -bu -F 4 | samtools sort -o {output.bam}) > /dev/zero"

rule bwa_mapping:
	input: 
		FR="output/trimmed/paired_end/{sample_paired}_val_1.fq.gz",
		RR="output/trimmed/paired_end/{sample_paired}_val_2.fq.gz"
	output:
		bam="output/mapping/paired_end/{sample_paired}.bam",
		bam_unmapped="output/mapping/paired_end/{sample_paired}_unmapped.bam"

	params: index=bwaindex,
		sample=lambda wildcards: wildcards.sample_paired
	threads: 4
	shell:
		"bwa mem -K 100000000 -Y {params.index} -t {threads} -R '@RG\\tID:{params.sample}\\tSM:{params.sample}'  {input.FR} {input.RR} |"
		"tee >(samtools view -f 4 -o {output.bam_unmapped}) >(samtools view -bu -F 4 | samtools sort -o {output.bam}) > /dev/zero"

rule bwa_gather:
	input:
		expand("output/mapping/paired_end/{sample}.bam",sample=sample_paired_end["sample"].drop_duplicates().values),
		expand("output/mapping/single_end/{sample}.bam",sample=sample_single_end["sample"].drop_duplicates().values)

	output:
		expand("output/mapping/{sample}.bam",sample=sample_dataframe["sample"].drop_duplicates().values)
	run:
		for f in input:
			basename = os.path.basename(f)
			os.symlink(os.path.join(os.path.basename(os.path.dirname(f)),basename), "output/mapping/{sample}".format(sample=basename))
rule bwa_index:
	input:
		"output/mapping/{sample}.bam"

	output:
		"output/mapping/{sample}.bam.bai"
	shell:
		"samtools index {input}"



rule markduplicates:
	input:
		bai="output/mapping/{sample}.bam.bai",
		bam="output/mapping/{sample}.bam"
	output:
		bam="output/mapping/{sample}.markduplicates.bam",
		metrics="output/reports/{sample}.markduplicates.metrics.txt"

	shell:
		"picard MarkDuplicates I={input.bam} O={output.bam} M={output.metrics} ASSUME_SORT_ORDER=coordinate"



#This takes te samples directly from markduplicates. As suggested, we may need to downsample first

rule downsample:
	input:
		bam=rules.markduplicates.output.bam
	output:
		bam="output/mapping/{sample}.downsample.bam"
	params:
		fraction=0.2
	shell:
		"picard DownsampleSam I={input.bam} O={output.bam} P={params.fraction} CREATE_INDEX=true"


rule bqsr:
	input:
		bam=rules.downsample.output.bam

	output:
		bqsr="output/bqsr/{sample}.bqsr"
	params:
		refernce_fasta="path_to_hg38",
		knownSites_dbsnp="{dbsnp_known_sites_vcf}",
		knownSites_mills="{mills_and_1000_gold_standard}",
		knownSites_indel="{hg38_known_indels}",
		optional_flags="--rf BadCigar --preserve_qscores_less_than 6",
		regions="-L chr1 -L chr2 -L chr3 -L chr4 -L chr5 -L chr6 -L chr7 -L chr8 -L chr9 -L chr10 -L chr11 -L chr12 -L chr13 -L chr14 -L chr15 -L chr16 -L chr17 -L chr18 -L chr19 -L chr20 -L chr21 -L chr22"
	shell:
		"gatk BaseRecalibrator -I {input.bam} -R {params.reference_fasta} -O {output.bqsr}"
		"--known-sites {params.knownSites_dbsnp} "
		"--known-sites {params.knownSites_dbsnp} "
		"--known-sites {params.known_Sites_indel} "
		"{params.optional_flags} "


rule apply_bqsr:
	input:
		bam=rules.markduplicates.output.bam,
		bqsr=rules.bqsr.output.bqsr
	params:
		reference="hg38_reference",
		optional_flags="--globalQScorePrior -1.0 --preserve_qscores_less_than 6 --useOriginalQualities --create-output-bam-index",
		compression_flags="-SQQ 10 -SQQ 20 -SQQ 30"
	output:
		bam="output/mapping/{sample}.recalibrated.bam"

	shell:
		"gatk ApplyBQSR -R {params.reference} -I {input.bam} -bqsr{input.bqsr} -O {output.bam} --disable_indel_quanls "
		"{params.optional_flags} {params.compression_flags}"	

rule haplotypecaller:
	input:
		bam=rules.apply_bqsr.output.bam
	params:
		reference="hg38_reference",
		other_options=" -G StandardAnnotation -G StandardHGAnnotation -G AS_StandardAnnotation"
		
	output:
		gvcf="output/haplotypecaller/{sample}.g.vcf.gz"
	
	
	shell:
		"gatk HaplotypeCaller -R {params.reference} -I {input.bam} -O {output.gvcf} {params.other_options}"



rule gcvfreblock:
	input:
		vcf=rules.haplotypecaller.output.gvcf,
	output:
		gvcfeblocked="output/haplotypecaller/{sample}.g.reblocked.vcf.gz"
	params:
		standard="-drop-low-quals -do-qual-approx --floor-blocks -GQB 10 -GWB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60",
		reference="hg38_reference"

	shell:
		"gatk ReblockGVCF -R {input.reference} -V {input.vcf} -O {gvcfreblocked}"


rule gvcf_samplemap:
	input:
		expand("output/haplotypecaller/{sample}.g.reblocked.vcf.gz", sample=sample_dataframe["sample"].drop_duplicates().values)
	output:
		"output/samplemap.tsv"
	run:
		with open("output/samplemap.tsv","w") as samplemapfile:
			for sample in sample_dataframe["sample"].drop_duplicates().values:
				samplemapfile.write("%s\%s\n"%(sample, "output/haplotypecaller/{sample}.g.reblocked.vcf.gz".format(sample=sample)))
	



rule genomicsdb:
	input:
		vcfmap=rules.gvcf_samplemap.output


	output:
		directory("output/genomicdb")
	params:
		standard=" --batch-size 50 -L 20 --reader-threads {threads} --merge-input-intervals --consolidate --tmp-dir /dev/shm/ "

	shell:
		"gatk GenomicsDBImport --genomicsdb-workspace-path {output} --sample-name-map {input} {params}"



rule genotypevcf:
	input:
		rules.genomicsdb.output
	output:
		"output/genotypes/genotypes.vcf.gz"

	params:
		standard=" -R hg38_reference -D dbsnp_file -G StandardAnnotation --only-output-calls-starting-in-intervals "
			 " --use-new-qual-calculator  -L interval --merge-input-intervals -V gendb://{input}"
	shell:
		"gatk GenotypeGVCFs {params.standard} -O {output}"

# fill in resource vcf files
rule vqsr:
	input:
		rules.genotypevcf.output
	params:
		standard=" -R hg38_reference --trust-all-polymorphic -trache ${sep=' -tranche ' recalibration_tranche_values}"
			 " -an ${sep=' -an ' recalibration_annotation_values} -mode SNP "
			 " --resource:hapmap,known=false,training=true,truth=true,prior=15 ${hapmap_resource_vcf} "
			 " --resource:omni,known=false,training=true,truth=true,prior=12 ${omni_resource_vcf} "
			 " --resource:1000G,known=false,training=true,truth=false,prior=10 ${one_thousand_genomes_resource_vcf} "
			 " --resource:dbsnp,known=true,training=false,truth=false,prior=7 ${dbsnp_resource_vcf} "
	output:
		recal="output/genotypes/genotypes.recal.vcf.gz",
		tranches="output/genotypes/tranches",
		model="output/genotypes/model.report"


rule applyvqsr:
	input:
		recal=rules.vqsr.output.recal,
		tranches=rules.vqsr.output.tranches,
		model=rules.vqsr.output.model,
		vcf=rules.genotypevcf.output

	params:
		standard="-R hg38_reference --recal-file {input.recal} --tranches-file {input.tranches} -mode SNP --truth-sensitivity-filter-level 0.99 --use-allele-specific-annotations --create-output-variant-index true"

	output:
		recalvcf="output/genotypes/genotypes.recal.vcf.gz"
	shell:
		"gatk ApplyVQSR {params.standard} -O {output.recalvcf} -V {input.vcf}"

		


	

	
	
		

		
